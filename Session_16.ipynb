{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Code for :\n",
    "- #### Gradient descent\n",
    "- #### Gradient descent implementation\n",
    "- #### Multilayer Perceptron\n",
    "- #### Backpropagation implementation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/7-essential-tips-for-writing-with-jupyter-notebook-60972a1a8901#78aa\n",
    "# https://github.com/dunovank/jupyter-themes#monospace-fonts-code-cells\n",
    "# !pip install jupyterthemes\n",
    "# !pip install --upgrade jupyterthemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !jt -t oceans16\n",
    "# !jt -t chesterish -tf ubuntu -tfs 13 -nf ptsans -nfs 14\n",
    "# !jt -t chesterish -tf ubuntu -tfs 13\n",
    "# !jt -t onedork -fs 95 -altp -tfs 18 -nfs 115 -cellw 88% -T\n",
    "!jt -t onedork -tf ubuntu -tfs 15 -nf ptsans -nfs 18 -cellw 100%\n",
    "# !jt -r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "As we discovered in few of our last sessions our first goal in creating neural network is to find the weights.\n",
    "With these weights we want to make sure our model makes predictions as close as possible to real values.\n",
    "But how would we will measure performance of model - here we will be using sum of squared errors (SSE)\n",
    "\n",
    "Which can be represented with mathematical equation:\n",
    "$E = \\frac{1}{2}\\sum_{\\mu}\\sum_{j}[{y_{j}}^{\\mu}-{\\hat{y}_{j}}^{\\mu}]^{2}$\n",
    "\n",
    "here $y$ is true value and $\\hat{y}$ is predicted value\n",
    "\n",
    "Now let's break and have a look at this equation\n",
    "\n",
    "1. Inside sum is saying for each output unit, find the difference between the true value $y$ and the predicted value from the network $\\hat{y}$ then square the difference, then sum up all those squares.\n",
    "2. Outer sum is over $\\mu$ is a sum over all the data points.\n",
    "\n",
    "We are using SSE because The square ensures the error is always positive and larger errors are penalized more than smaller errors.\n",
    "\n",
    "Remember that to minimize the squared Error we need to change weights of our model, thus we simply go back to __gradient descent__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already know what Gradient Descent is - but here to oversimplify it I will say Gradient is another term for rate of change or slope.\n",
    "To calculate rate of change we need to find derivatives of that point.\n",
    "\n",
    "A derivative of a function gives us the slope\n",
    "For example\n",
    "$f(x) = x^2$\n",
    "$f'(x) = 2x$\n",
    "thus if $x =2$ the slope $f'(2) = 4$\n",
    "\n",
    "One caveat of Gradient descent is that weights can be decreased to a point where error is low, but not the lowest, these spots are called local minima.\n",
    "There are methods to mitigate this called momentum - you can read about it [here](https://distill.pub/2017/momentum/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U pip\n",
    "!pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"Calculate Sigmoid\"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    \"\"\"Derivative of the sigmoid function\"\"\"\n",
    "    return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "# We can create an array of input values\n",
    "x = np.array([0.1, 0.3])\n",
    "\n",
    "# Our expected output\n",
    "y = 0.2\n",
    "\n",
    "# Initial weights\n",
    "w = np.array([-0.8, 0.5])\n",
    "\n",
    "# The learning rate which should always be between 0-1\n",
    "learnrate = 0.5\n",
    "\n",
    "# the linear combination performed by the node (h in f(h) and f'(h))\n",
    "# we take a dot product of our inputs and weights\n",
    "h = np.dot(x, w)\n",
    "\n",
    "# predicted output from our network (y-hat)\n",
    "nn_output = sigmoid(h)\n",
    "\n",
    "# output error (y - y-hat)\n",
    "error = y - nn_output\n",
    "\n",
    "# output gradient (f'(h))\n",
    "output_grad = sigmoid_prime(h)\n",
    "\n",
    "# error term (lowercase delta)\n",
    "error_term = error * output_grad\n",
    "\n",
    "# Gradient descent step\n",
    "del_w = learnrate * error_term * x\n",
    "\n",
    "print(f'Neural Network output: {nn_output} in percentage {nn_output:.1%}')\n",
    "print(f'Amount of Error: {error} in percentage {error:.1%}')\n",
    "print(f'Change in Weights: {del_w} ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent implementation\n",
    "\n",
    "Code above gives us an idea about how single weight update is implemented, but what about weight update on whole network.\n",
    "\n",
    "Let us take another example which uses school admissions data from UCLA.\n",
    "This dataset has three inputs\n",
    "GRE score, GPA and rank of undergrad school (where rank 1 is for highest prestige and 4 is lowest)\n",
    "\n",
    "Our goal is to predict old age question - if student should be admitted to graduate program or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data cleanup\n",
    "\n",
    "After checking the CSV file, we see that there are 4 columns \"admit\", \"gre\", \"gpa\", \"rank\"\n",
    "Now the rank data is not in normalized for here - so let's break them down into dummy variables/columns with 0 and 1\n",
    "Besides this gre and gpa needs also to be prepared - both need to have zero mean and SD of 1.\n",
    "This step is important because of our sigmoid function which if you remember reduces very small and very large values near to zero, which in turn make our descent go to zero too.\n",
    "\n",
    "#### Mean Square Error\n",
    "\n",
    "To get our error calculated we will make a change from sum of squared errors to mean of squared errors, becuase we are using a large dataset and summing all of it will result in a\n",
    "large change for gradient descent. We can do compensation by using a very small learning rate but that won't be effective alternative.\n",
    "Therefore, we will be taking mean and keeping our learning rate in range of 0.01 to 0.001\n",
    "\n",
    "Now mean square error can be equated as :\n",
    "\n",
    "$E = \\frac{1}{2m}\\sum_{\\mu}[{y^{\\mu}-{\\hat{y}^{\\mu}]^{2}$\n",
    "\n",
    "Generally these steps are followed for updating weights through gradient descent:\n",
    "\n",
    "1. set the weight step to zero $\\Delta w_i = 0$\n",
    "2. loop through each record in training data:\n",
    "        1. Move forward through network and calculate output $\\hat{y} = f(\\sum_iw_ix_i)$\n",
    "        2. Calculate the error term for the output unit $\\delta = (y - \\hat{y}) * f'(\\sum_iw_ix_i)$\n",
    "        2. update the weight step $Δw_i = Δw_i + \\delta x_i$\n",
    "3. Update the weights $w_i = \\frac{w_i + \\eta \\Delta w_i}{m}$ here $\\eta$ is learning rate and $m$ is number of records.\n",
    "4. Repeat for $e$ epochs\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$E=\\frac{1}{2m}\\sum_{\\mu}[{y^{\\mu}-{\\hat{y}^{\\mu}]^{2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_records: 360\n",
      "n_features: 6\n",
      "Train loss:  0.2627609384996635\n",
      "Train loss:  0.20928619409324875\n",
      "Train loss:  0.20084292908073426\n",
      "Train loss:  0.19862156475527873\n",
      "Train loss:  0.1977985139668603\n",
      "Train loss:  0.19742577912189868\n",
      "Train loss:  0.1972350774624106\n",
      "Train loss:  0.1971294562509248\n",
      "Train loss:  0.19706766341315082\n",
      "Train loss:  0.19703005801777368\n",
      "Prediction accuracy: 0.725 in percentage 72.50%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## read csv into a var\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "# remove the rank column\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standardize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "\n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(42)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "\n",
    "## Implementing the Gradient Descent here\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# TODO: We haven't provided the sigmoid_prime function like we did in\n",
    "#       the previous lesson to encourage you to come up with a more\n",
    "#       efficient solution. If you need a hint, check out the comments\n",
    "#       in solution.py from the previous lecture.\n",
    "\n",
    "# Use to same seed to make debugging easier\n",
    "np.random.seed(42)\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "print(f'n_records: {n_records}')\n",
    "print(f'n_features: {n_features}')\n",
    "last_loss = None\n",
    "\n",
    "# Initialize weights randomly so that they are diverged and asymmetrical\n",
    "weights = np.random.normal(scale=1 / n_features**.5, size=n_features)\n",
    "\n",
    "# Epochs or iterations and learnrate are known as hyperparameters of neural network\n",
    "epochs = 1000\n",
    "learnrate = 0.5\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w = np.zeros(weights.shape)\n",
    "    # We want to loop through all the data keeping x as input, y as target\n",
    "    for x, y in zip(features.values, targets):\n",
    "\n",
    "        # np.dot is used to get product of two arrays - which is our h activation function\n",
    "        output = sigmoid(np.dot(x, weights))\n",
    "\n",
    "        # The error, the target minus the network output\n",
    "        error = y - output\n",
    "\n",
    "        # instead of creating another function we calculate faster here and store in a reusable var\n",
    "        error_term = error * output * (1 - output)\n",
    "\n",
    "\n",
    "        # The gradient descent step, the error times the gradient times the inputs\n",
    "        del_w += error_term * x\n",
    "\n",
    "    # Update the weights here. The learning rate times the\n",
    "    # change in weights, divided by the number of records to average\n",
    "    weights += learnrate * del_w / n_records\n",
    "    # print(f\"Weights: {weights}\")\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        out = sigmoid(np.dot(features, weights))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "tes_out = sigmoid(np.dot(features_test, weights))\n",
    "predictions = tes_out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(f\"Prediction accuracy: {accuracy:.3f} in percentage {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "#### Implementing the hidden layer\n",
    "\n",
    "Now we need to deal with multiple input and multiple hidden units, thus the weights between them will require\n",
    "two indices $w_ij$ where $i$ denotes input units and $j$ are hidden units.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden-layer Output: [0.41492192 0.42604313 0.5002434 ]\n",
      "Output-layer Output: [0.49815196 0.48539772]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "# Network size\n",
    "N_input = 4\n",
    "N_hidden = 3\n",
    "N_output = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "# Make some fake data\n",
    "X = np.random.randn(4)\n",
    "\n",
    "weights_input_to_hidden = np.random.normal(0, scale=0.1, size=(N_input, N_hidden))\n",
    "weights_hidden_to_output = np.random.normal(0, scale=0.1, size=(N_hidden, N_output))\n",
    "\n",
    "\n",
    "# TODO: Make a forward pass through the network\n",
    "\n",
    "hidden_layer_in = np.dot(X, weights_input_to_hidden)\n",
    "hidden_layer_out = sigmoid(hidden_layer_in)\n",
    "\n",
    "print(f'Hidden-layer Output: {hidden_layer_out}')\n",
    "\n",
    "output_layer_in = np.dot(hidden_layer_out, weights_hidden_to_output)\n",
    "output_layer_out = sigmoid(output_layer_in)\n",
    "\n",
    "print(f'Output-layer Output: {output_layer_out}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing backpropagation\n",
    "\n",
    "The backpropagation algorithm is just an extension of weight update using gradient descent, where we use chain rule to find the errors with respect to weights connecting hidden and input layers.\n",
    "\n",
    "Let's implement the algorithm for graduate admission school data.\n",
    "We want to\n",
    "- Implement the forward pass.\n",
    "- Implement the backpropagation algorithm.\n",
    "- Update the weights.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss:  0.29135986165965394\n",
      "Train loss:  0.2893218729151472\n",
      "Train loss:  0.2873191667944074\n",
      "Train loss:  0.28535246721671065\n",
      "Train loss:  0.283422431956647\n",
      "Train loss:  0.28152964950581233\n",
      "Train loss:  0.2796746365249894\n",
      "Train loss:  0.27785783590588\n",
      "Train loss:  0.2760796154508361\n",
      "Train loss:  0.2743402671686853\n",
      "Prediction accuracy: 0.425 in percentage: 42.50%\n"
     ]
    }
   ],
   "source": [
    "#### ------ DATA PREP --------- ####\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "## read csv into a var\n",
    "admissions = pd.read_csv('binary.csv')\n",
    "\n",
    "# Make dummy variables for rank\n",
    "data = pd.concat([admissions, pd.get_dummies(admissions['rank'], prefix='rank')], axis=1)\n",
    "# remove the rank column\n",
    "data = data.drop('rank', axis=1)\n",
    "\n",
    "# Standardize features\n",
    "for field in ['gre', 'gpa']:\n",
    "    mean, std = data[field].mean(), data[field].std()\n",
    "    data.loc[:,field] = (data[field]-mean)/std\n",
    "\n",
    "# Split off random 10% of the data for testing\n",
    "np.random.seed(80)\n",
    "sample = np.random.choice(data.index, size=int(len(data)*0.9), replace=False)\n",
    "data, test_data = data.iloc[sample], data.drop(sample)\n",
    "\n",
    "# Split into features and targets\n",
    "features, targets = data.drop('admit', axis=1), data['admit']\n",
    "features_test, targets_test = test_data.drop('admit', axis=1), test_data['admit']\n",
    "\n",
    "#### ------ IMPLEMENTATION --------- ####\n",
    "\n",
    "# remember selecting seed also is important - if you change the seed you will difference in accuracy.\n",
    "# np.random.seed(25)\n",
    "\n",
    "def sigmoid(x):\n",
    "    \"\"\"\n",
    "    Calculate sigmoid\n",
    "    \"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "n_hidden = 2  # number of hidden units\n",
    "epochs = 900\n",
    "learnrate = 0.005\n",
    "\n",
    "n_records, n_features = features.shape\n",
    "last_loss = None\n",
    "# Initialize weights\n",
    "weights_input_hidden = np.random.normal(scale=1 / n_features ** .5, size=(n_features, n_hidden))\n",
    "weights_hidden_output = np.random.normal(scale=1 / n_features ** .5, size=n_hidden)\n",
    "\n",
    "for e in range(epochs):\n",
    "    del_w_input_hidden = np.zeros(weights_input_hidden.shape)\n",
    "    del_w_hidden_output = np.zeros(weights_hidden_output.shape)\n",
    "    for x, y in zip(features.values, targets):\n",
    "        ## Forward pass ##\n",
    "        hidden_input = np.dot(x, weights_input_hidden)\n",
    "        hidden_output = sigmoid(hidden_input)\n",
    "\n",
    "        output = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "\n",
    "        ## Backward pass ##\n",
    "        error = y - output\n",
    "        output_error_term = error * output * (1 - output)\n",
    "\n",
    "        ## propagate errors to hidden layer\n",
    "        hidden_error = np.dot(output_error_term, weights_hidden_output)\n",
    "        hidden_error_term = hidden_error * hidden_output * (1 - hidden_output)\n",
    "\n",
    "        del_w_hidden_output += output_error_term * hidden_output\n",
    "        del_w_input_hidden += hidden_error_term * x[:, None]\n",
    "\n",
    "\n",
    "    weights_input_hidden += learnrate * del_w_input_hidden / n_records\n",
    "    weights_hidden_output += learnrate * del_w_hidden_output / n_records\n",
    "\n",
    "    # Printing out the mean square error on the training set\n",
    "    if e % (epochs / 10) == 0:\n",
    "        hidden_output = sigmoid(np.dot(x, weights_input_hidden))\n",
    "        out = sigmoid(np.dot(hidden_output, weights_hidden_output))\n",
    "        loss = np.mean((out - targets) ** 2)\n",
    "\n",
    "        if last_loss and last_loss < loss:\n",
    "            print(\"Train loss: \", loss, \"  WARNING - Loss Increasing\")\n",
    "        else:\n",
    "            print(\"Train loss: \", loss)\n",
    "        last_loss = loss\n",
    "\n",
    "# Calculate accuracy on test data\n",
    "hidden = sigmoid(np.dot(features_test, weights_input_hidden))\n",
    "out = sigmoid(np.dot(hidden, weights_hidden_output))\n",
    "predictions = out > 0.5\n",
    "accuracy = np.mean(predictions == targets_test)\n",
    "print(f\"Prediction accuracy: {accuracy:.3f} in percentage: {accuracy:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
